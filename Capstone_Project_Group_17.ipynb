{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio langchain transformers bertopic torch langchain-community langchain_groq amadeus langchain_huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSXXc4H4Rv7f",
        "outputId": "c60b7957-81c1-4ba5-83e4-f45dd0ad435c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.9.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Collecting bertopic\n",
            "  Downloading bertopic-0.16.4-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.13-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.2.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting amadeus\n",
            "  Downloading amadeus-11.0.0.tar.gz (38 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.5.2 (from gradio)\n",
            "  Downloading gradio_client-1.5.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.8.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.42.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (14.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.25)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Collecting hdbscan>=0.8.29 (from bertopic)\n",
            "  Downloading hdbscan-0.8.40-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.6.0)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (3.3.1)\n",
            "Collecting umap-learn>=0.5.0 (from bertopic)\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.13-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.25 (from langchain)\n",
            "  Downloading langchain_core-0.3.28-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.7.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.13.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.23.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain) (3.0.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.9.1-py3-none-any.whl (57.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bertopic-0.16.4-py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m143.7/143.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.13-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.13-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_groq-0.2.2-py3-none-any.whl (14 kB)\n",
            "Downloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.13.1-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hdbscan-0.8.40-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.28-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.8.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading marshmallow-3.23.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: amadeus\n",
            "  Building wheel for amadeus (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for amadeus: filename=amadeus-11.0.0-py2.py3-none-any.whl size=71670 sha256=bbe72002ec443f51cd972abda160351e6745bf6537fae6317052af077fb1a017\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/fd/17/4bc056ced9da64f376c56d0684bcf7545413cdfcb0080460c7\n",
            "Successfully built amadeus\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, python-dotenv, mypy-extensions, marshmallow, markupsafe, httpx-sse, ffmpy, amadeus, aiofiles, typing-inspect, starlette, safehttpx, pynndescent, pydantic-settings, hdbscan, groq, gradio-client, fastapi, dataclasses-json, umap-learn, langchain-core, gradio, langchain_groq, langchain_huggingface, langchain, bertopic, langchain-community\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.25\n",
            "    Uninstalling langchain-core-0.3.25:\n",
            "      Successfully uninstalled langchain-core-0.3.25\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.12\n",
            "    Uninstalling langchain-0.3.12:\n",
            "      Successfully uninstalled langchain-0.3.12\n",
            "Successfully installed aiofiles-23.2.1 amadeus-11.0.0 bertopic-0.16.4 dataclasses-json-0.6.7 fastapi-0.115.6 ffmpy-0.5.0 gradio-5.9.1 gradio-client-1.5.2 groq-0.13.1 hdbscan-0.8.40 httpx-sse-0.4.0 langchain-0.3.13 langchain-community-0.3.13 langchain-core-0.3.28 langchain_groq-0.2.2 langchain_huggingface-0.1.2 markupsafe-2.1.5 marshmallow-3.23.2 mypy-extensions-1.0.0 pydantic-settings-2.7.0 pydub-0.25.1 pynndescent-0.5.13 python-dotenv-1.0.1 python-multipart-0.0.20 ruff-0.8.4 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.13.2 typing-inspect-0.9.0 umap-learn-0.5.7 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2j8GjFIRQFx"
      },
      "outputs": [],
      "source": [
        "from amadeus import Client, ResponseError\n",
        "import os\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "import gradio as gr\n",
        "import time\n",
        "import sys\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# sys.path.append('E:/Personal/IISC/Capstone/IISC_Capstone_Project/')\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import time\n",
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ie2JzNH-SS4K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c439b78c-9aff-4570-af86-1d99493c175b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/IISC_Capstone_Project')\n",
        "from input_analysis_agent import InputAnalysisAgent\n",
        "from response_agent import ResponseAgent\n",
        "from data_retrival_agent import TravelPlanningAgent\n",
        "from performance_evaluation_agent import PerformanceEvaluationAgent"
      ],
      "metadata": {
        "id": "ciTstBMHSbmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_css = \"\"\"\n",
        ".travel-chatbot-heading {\n",
        "    text-align: center;\n",
        "    padding: 20px;\n",
        "    margin-bottom: 20px;\n",
        "    background: #4169E1;  /* Royal Blue */\n",
        "    color: white;\n",
        "    text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);\n",
        "    border-radius: 10px;\n",
        "    font-family: 'Arial', sans-serif;\n",
        "    font-size: 36px;\n",
        "    font-weight: bold;\n",
        "    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hob6HHPrSxjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_final_response_feedback = \"-\"\n",
        "global_sentiment_feedback = \"-\"\n",
        "global_intent_feedback = \"-\"\n",
        "global_topic_feedback = \"-\"\n",
        "global_current_response_feedback = \"-\"\n",
        "\n",
        "# Global variables for storing previous conversation\n",
        "global_prev_question = \"empty\"\n",
        "global_prev_sentiment = \"empty\"\n",
        "global_prev_intent = \"empty\"\n",
        "global_prev_topic = \"empty\"\n",
        "global_prev_answer_str = \"empty\"\n",
        "global_prev_response_time = \"empty\""
      ],
      "metadata": {
        "id": "HedAZCcOS1m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMQuestionRefiner:\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "        self.memory = ConversationBufferMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            return_messages=True\n",
        "        )\n",
        "\n",
        "        self.refine_template = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a LLMQuestionRefiner as a user assistance that refines my input by adding relevant context\n",
        "            from previous conversation. Create a single, clear, and concise input that includes important\n",
        "            context from the chat history. Do not add explanations or additional text.\n",
        "\n",
        "            Rules:\n",
        "            1. Only output the refined question, nothing else\n",
        "            2. Keep the question natural and straight forward\n",
        "            3. Only add context that is directly relevant\n",
        "            4. If no relevant context exists, return the original question unchanged\n",
        "            5. Don't make assumptions about context that isn't in the history\n",
        "\n",
        "            Example 1:\n",
        "            History: \"I'm planning a trip to Paris next month\"\n",
        "            Question: \"What museums should I visit?\"\n",
        "            Output: \"What museums should I visit in Paris?\"\n",
        "\n",
        "            Example 2:\n",
        "            History: \"\"\n",
        "            Question: \"I am planning a trip to Mumbai\"\n",
        "            Output: \"I am planning a trip to Mumbai\"\n",
        "\n",
        "            Example 2:\n",
        "            History: \"I love India\"\n",
        "            Question: \"I am planning a trip to Mumbai\"\n",
        "            Output: \"I am planning a trip to Mumbai\"\n",
        "            \"\"\"),\n",
        "            (\"human\", \"\"\"Previous conversation:\n",
        "            {chat_history}\n",
        "\n",
        "            Current question: {question}\"\"\")\n",
        "        ])\n",
        "\n",
        "    def add_message(self, message: str, is_user: bool = True):\n",
        "        \"\"\"Add a message to conversation history\"\"\"\n",
        "        if is_user:\n",
        "            self.memory.chat_memory.add_user_message(message)\n",
        "        else:\n",
        "            self.memory.chat_memory.add_ai_message(message)\n",
        "\n",
        "    def refine_question(self, question: str) -> str:\n",
        "        \"\"\"Refine the question using LLM and conversation context\"\"\"\n",
        "        history = self.memory.load_memory_variables({})[\"chat_history\"]\n",
        "        chain = self.refine_template | self.llm\n",
        "        result = chain.invoke({\n",
        "            \"chat_history\": history,\n",
        "            \"question\": question\n",
        "        })\n",
        "        return result.content\n",
        "\n",
        "    def clear_memory(self):\n",
        "        \"\"\"Clear all conversation history\"\"\"\n",
        "        self.memory.clear()"
      ],
      "metadata": {
        "id": "4QOeJaJY4FEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "if __name__ == \"__main__\":\n",
        "    amadeus = Client(\n",
        "        client_id=\"QBSBUsh1p6DwLRtF2KpDavFECGYvZ4Ui\",\n",
        "        client_secret=\"fL4XnNYLFeitqO4I\",\n",
        "    )\n",
        "\n",
        "    # token = os.getenv(\"GROQ_API_KEY\")\n",
        "    # llm = ChatGroq(\n",
        "    #     temperature=0.1,\n",
        "    #     model_name=\"mixtral-8x7b-32768\",\n",
        "    #     groq_api_key=token,\n",
        "    #     max_tokens=1024\n",
        "    # )\n",
        "    os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "    token_groq = os.getenv(\"GROQ_API_KEY\")\n",
        "    llm_groq = ChatGroq(\n",
        "        temperature=0.1,\n",
        "        model_name=\"mixtral-8x7b-32768\",\n",
        "        groq_api_key=token_groq,\n",
        "        max_tokens=1024\n",
        "    )\n",
        "\n",
        "    os.environ[\"OA_Key\"] = userdata.get('OA_Key')\n",
        "    token = os.environ.get(\"OA_Key\")\n",
        "    llm = ChatOpenAI(temperature=1, model=\"gpt-3.5-turbo\", openai_api_key=token)\n",
        "\n",
        "    response_agent = ResponseAgent(llm)\n",
        "\n",
        "    input_analysis_agent = InputAnalysisAgent(\"/content/drive/MyDrive/IISC_Capstone_Project/id_to_label_intent.csv\", \"/content/drive/MyDrive/IISC_Capstone_Project/id_to_label_sentiment.csv\", \"/content/drive/MyDrive/IISC_Capstone_Project/id_to_label_topic.csv\")\n",
        "    input_analysis_agent.InitializeAnalyzers(\"/content/drive/MyDrive/IISC_Capstone_Project/models/fine_tuned_distilbert_intent\", \"/content/drive/MyDrive/IISC_Capstone_Project/models/fine_tuned_distilbert_sentiment\", \"/content/drive/MyDrive/IISC_Capstone_Project/models/fine_tuned_distilbert_topic\")\n",
        "    input_analysis_agent.CreateTools()\n",
        "    input_analysis_agent.InitializeAgent(llm)\n",
        "\n",
        "    data_retrival_agent = TravelPlanningAgent(llm, amadeus)\n",
        "\n",
        "    performance_evaluation_agent = PerformanceEvaluationAgent(\"/content/drive/MyDrive/IISC_Capstone_Project/conversation_history.json\", llm_groq)\n",
        "\n",
        "    refiner = LLMQuestionRefiner(llm_groq)\n",
        "\n",
        "    # memory = ConversationBufferMemory(input_key='query')\n",
        "\n",
        "\n",
        "    def analyze_question(question, history):\n",
        "        \"\"\"\n",
        "        Analyzes the user question and returns relevant information\n",
        "        \"\"\"\n",
        "        try:\n",
        "            global global_prev_question, global_prev_sentiment, global_prev_intent\n",
        "            global global_prev_topic, global_prev_answer_str, global_prev_response_time\n",
        "\n",
        "            global global_final_response_feedback, global_sentiment_feedback, global_intent_feedback, global_topic_feedback, global_current_response_feedback\n",
        "\n",
        "            # Save previous conversation if exists\n",
        "            if global_prev_question != \"empty\":\n",
        "                save_to_json(\n",
        "                    global_prev_question,\n",
        "                    global_prev_sentiment,\n",
        "                    global_prev_intent,\n",
        "                    global_prev_topic,\n",
        "                    global_prev_answer_str,\n",
        "                    global_prev_response_time\n",
        "                )\n",
        "\n",
        "            # final_rating_output = performance_evaluation_agent.RateFromJson()\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            refiner.add_message(question, True)\n",
        "            refined_question = refiner.refine_question(question)\n",
        "            print(\"\\n\\n REFINED QUESTION: \", refined_question)\n",
        "\n",
        "            # Get the analysis results\n",
        "            response = input_analysis_agent.Execute(refined_question)\n",
        "            intent = response.split(\"Intent: \")[1].split(\",\")[0]\n",
        "            intent = intent.replace(\"_\", \" \").replace(\"\\\\\", \"\").title()\n",
        "\n",
        "            sentiment = response.split(\"Sentiment: \")[1].split(\",\")[0]\n",
        "            sentiment = sentiment.replace(\"_\", \" \").replace(\"\\\\\", \"\").title()\n",
        "\n",
        "            topic = response.split(\"Topic: \")[1]\n",
        "            topic = topic.replace(\"_\", \" \").replace(\"\\\\\", \"\").title()\n",
        "\n",
        "            # Get context and generate response\n",
        "\n",
        "            context = data_retrival_agent.run(intent, refined_question)\n",
        "            print(\"\\n\\n CONTEXT: \", context)\n",
        "            answer = response_agent.GenerateResponse(refined_question, context)\n",
        "\n",
        "            # refiner.add_message(answer, False)\n",
        "\n",
        "            # Calculate response time\n",
        "            response_time = int((time.time() - start_time) * 1000)\n",
        "\n",
        "\n",
        "            # Format history properly for Gradio Chatbot\n",
        "            history = history or []\n",
        "            # Ensure the answer is a string\n",
        "            answer_str = str(answer)\n",
        "            history.append([question, answer_str])\n",
        "\n",
        "            global_final_response_feedback = \"-\"\n",
        "            global_sentiment_feedback = \"-\"\n",
        "            global_intent_feedback = \"-\"\n",
        "            global_topic_feedback = \"-\"\n",
        "            global_current_response_feedback = \"-\"\n",
        "\n",
        "            # Store current conversation for next save\n",
        "            global_prev_question = question\n",
        "            global_prev_sentiment = sentiment\n",
        "            global_prev_intent = intent\n",
        "            global_prev_topic = topic\n",
        "            global_prev_answer_str = answer_str\n",
        "            global_prev_response_time = response_time\n",
        "\n",
        "            return (\n",
        "                \"\",\n",
        "                history,      # Properly formatted history\n",
        "                sentiment,\n",
        "                intent,\n",
        "                topic,\n",
        "                answer_str,   # Ensure we return the string version\n",
        "                response_time,\n",
        "                refined_question,\n",
        "                final_rating_output\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error in analyze_question: {str(e)}\")\n",
        "\n",
        "            # Return empty/error values in case of failure\n",
        "            return (\n",
        "                \"\",\n",
        "                history or [],\n",
        "                \"Error\",\n",
        "                \"Error\",\n",
        "                \"Error\",\n",
        "                f\"An error occurred: {str(e)}\",\n",
        "                0,\n",
        "                \"Error\",\n",
        "                \"Error\"\n",
        "            )\n",
        "\n",
        "    def clear_inputs(questions_chain):\n",
        "        \"\"\"\n",
        "        Clears all input and output fields and stores last feedback in json\n",
        "        \"\"\"\n",
        "        global global_prev_question, global_prev_sentiment, global_prev_intent\n",
        "        global global_prev_topic, global_prev_answer_str, global_prev_response_time\n",
        "\n",
        "        # Save previous conversation if exists\n",
        "        if global_prev_question != \"empty\":\n",
        "            save_to_json(\n",
        "                global_prev_question,\n",
        "                global_prev_sentiment,\n",
        "                global_prev_intent,\n",
        "                global_prev_topic,\n",
        "                global_prev_answer_str,\n",
        "                global_prev_response_time\n",
        "            )\n",
        "        # refiner.clear_memory()\n",
        "        # memory.clear()\n",
        "        questions_chain = \"\"\n",
        "        return (\n",
        "            \"\",          # Clear question input\n",
        "            [],          # Clear chat history - changed from None to []\n",
        "            \"\",          # Clear sentiment\n",
        "            \"\",          # Clear intent\n",
        "            \"\",          # Clear topic\n",
        "            \"\",          # Clear answer\n",
        "            \"\",          # Clear response time\n",
        "            \"\",          # Clear final rating\n",
        "            \"\"\n",
        "        )\n",
        "\n",
        "    def end_conversation(questions_chain):\n",
        "        \"\"\"\n",
        "        Clears all input and output fields and stores last feedback in json and display final rating\n",
        "        \"\"\"\n",
        "        global global_prev_question, global_prev_sentiment, global_prev_intent\n",
        "        global global_prev_topic, global_prev_answer_str, global_prev_response_time\n",
        "\n",
        "        # Save previous conversation if exists\n",
        "        if global_prev_question != \"empty\":\n",
        "            save_to_json(\n",
        "                global_prev_question,\n",
        "                global_prev_sentiment,\n",
        "                global_prev_intent,\n",
        "                global_prev_topic,\n",
        "                global_prev_answer_str,\n",
        "                global_prev_response_time\n",
        "            )\n",
        "        questions_chain = \"\"\n",
        "        final_rating_output = performance_evaluation_agent.RateFromJson()\n",
        "        return (\n",
        "            \"\",          # Clear question input\n",
        "            \"\",             #Clear refined question\n",
        "            [],          # Clear chat history - changed from None to []\n",
        "            \"\",          # Clear sentiment\n",
        "            \"\",          # Clear intent\n",
        "            \"\",          # Clear topic\n",
        "            \"\",          # Clear answer\n",
        "            \"\",          # Clear response time\n",
        "            final_rating_output\n",
        "        )\n",
        "\n",
        "    def capture_feedback_final_response(component_type, feedback_value):\n",
        "        \"\"\"\n",
        "        Captures and processes user feedback\n",
        "        \"\"\"\n",
        "        global global_final_response_feedback\n",
        "        global_final_response_feedback = feedback_value\n",
        "        print(f\"Received {component_type} feedback: {feedback_value}\")\n",
        "        return f\"Thank you for your {feedback_value} feedback on {component_type}!\"\n",
        "\n",
        "    def capture_feedback_sentiment(component_type, feedback_value):\n",
        "        \"\"\"\n",
        "        Captures and processes user feedback\n",
        "        \"\"\"\n",
        "        global global_sentiment_feedback\n",
        "        global_sentiment_feedback = feedback_value\n",
        "        print(f\"Received {component_type} feedback: {feedback_value}\")\n",
        "        return f\"Thank you for your {feedback_value} feedback on {component_type}!\"\n",
        "\n",
        "    def capture_feedback_intent(component_type, feedback_value):\n",
        "        \"\"\"\n",
        "        Captures and processes user feedback\n",
        "        \"\"\"\n",
        "        global global_intent_feedback\n",
        "        global_intent_feedback = feedback_value\n",
        "        print(f\"Received {component_type} feedback: {feedback_value}\")\n",
        "        return f\"Thank you for your {feedback_value} feedback on {component_type}!\"\n",
        "\n",
        "    def capture_feedback_topic(component_type, feedback_value):\n",
        "        \"\"\"\n",
        "        Captures and processes user feedback\n",
        "        \"\"\"\n",
        "        global global_topic_feedback\n",
        "        global_topic_feedback = feedback_value\n",
        "        print(f\"Received {component_type} feedback: {feedback_value}\")\n",
        "        return f\"Thank you for your {feedback_value} feedback on {component_type}!\"\n",
        "\n",
        "    def capture_feedback_current_response(component_type, feedback_value):\n",
        "        \"\"\"\n",
        "        Captures and processes user feedback\n",
        "        \"\"\"\n",
        "        global global_current_response_feedback\n",
        "        global_current_response_feedback = feedback_value\n",
        "        print(f\"Received {component_type} feedback: {feedback_value}\")\n",
        "        return f\"Thank you for your {feedback_value} feedback on {component_type}!\"\n",
        "\n",
        "    def save_to_json(question, sentiment, intent, topic, answer, response_time):\n",
        "        global global_final_response_feedback, global_sentiment_feedback, global_intent_feedback, global_topic_feedback, global_current_response_feedback\n",
        "\n",
        "        data = {\n",
        "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"conversation_data\": {\n",
        "                \"question\": question,\n",
        "                \"sentiment\": sentiment,\n",
        "                \"intent\": intent,\n",
        "                \"topic\": topic,\n",
        "                \"answer\": answer,\n",
        "                \"response_time\": response_time,\n",
        "                \"final_response_feedback\": global_final_response_feedback,\n",
        "                \"sentiment_feedback\": global_sentiment_feedback,\n",
        "                \"intent_feedback\": global_intent_feedback,\n",
        "                \"topic_feedback\": global_topic_feedback,\n",
        "                \"current_response_feedback\": global_current_response_feedback,\n",
        "            },\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            with open('/content/drive/MyDrive/IISC_Capstone_Project/conversation_history.json', 'r') as f:\n",
        "                existing_data = json.load(f)\n",
        "        except (FileNotFoundError, json.JSONDecodeError):\n",
        "            existing_data = []\n",
        "\n",
        "        existing_data.append(data)\n",
        "\n",
        "        with open('/content/drive/MyDrive/IISC_Capstone_Project/conversation_history.json', 'w') as f:\n",
        "            json.dump(existing_data, f, indent=4)\n",
        "\n",
        "\n",
        "    # Create the Gradio interface\n",
        "    with gr.Blocks(css=custom_css) as demo:\n",
        "        # Artistic Heading\n",
        "        # gr.HTML(\n",
        "        #     \"\"\"\n",
        "        #     <div class=\"travel-chatbot-heading\">\n",
        "        #         ‚úàÔ∏è RoamRight : Your AI Travel Companion üåé\n",
        "        #     </div>\n",
        "        #     \"\"\"\n",
        "        # )\n",
        "        gr.HTML(\n",
        "            \"\"\"\n",
        "            <div class=\"travel-chatbot-container\">\n",
        "                <div class=\"travel-chatbot-heading\">\n",
        "                    ‚úàÔ∏è RoamRight : Your AI Travel Companion üåé\n",
        "                </div>\n",
        "                <div class=\"travel-chatbot-subheading\">\n",
        "                    Got questions about boarding, lodging, or travel? Let‚Äôs journey together‚Äîdon‚Äôt forget to rate us!\n",
        "                </div>\n",
        "                <style>\n",
        "                    .travel-chatbot-container {\n",
        "                        text-align: center;\n",
        "                        padding: 20px 0;\n",
        "                    }\n",
        "                    .travel-chatbot-heading {\n",
        "                        font-size: 24px;\n",
        "                        font-weight: bold;\n",
        "                        margin-bottom: 10px;\n",
        "                    }\n",
        "                    .travel-chatbot-subheading {\n",
        "                        font-size: 18px;\n",
        "                        color: #666;\n",
        "                        font-style: bold;\n",
        "                    }\n",
        "                </style>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            # Left Panel\n",
        "            with gr.Column(scale=1):\n",
        "                question_input = gr.Textbox(\n",
        "                    label=\"Your Question\",\n",
        "                    placeholder=\"Ask your travel-related question here...\"\n",
        "                )\n",
        "                refined_question = gr.Textbox(\n",
        "                    label=\"Refined Question\",\n",
        "                    interactive=False\n",
        "                )\n",
        "                chatbot = gr.Chatbot(\n",
        "                    label=\"Conversation History\",\n",
        "                    height=300\n",
        "                )\n",
        "                with gr.Row():\n",
        "                    chat_thumbs_up = gr.Button(\"üëç\")\n",
        "                    chat_thumbs_down = gr.Button(\"üëé\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    submit_btn = gr.Button(\"Submit\")\n",
        "                    clear_btn = gr.Button(\"Clear\")\n",
        "\n",
        "                final_rating_output = gr.Textbox(\n",
        "                    label=\"Auto Performance Evaluation\",\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    end_conversation_btn = gr.Button(\"End Conversation\", scale=2)\n",
        "\n",
        "            # Right Panel\n",
        "            with gr.Column(scale=1):\n",
        "                # Response Time Display\n",
        "                response_time_output = gr.Textbox(\n",
        "                    label=\"Response Time (ms)\",\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "                # Sentiment Row\n",
        "                sentiment_output = gr.Textbox(\n",
        "                    label=\"Sentiment\",\n",
        "                    interactive=False\n",
        "                )\n",
        "                with gr.Row():\n",
        "                    sentiment_up = gr.Button(\"üëç\")\n",
        "                    sentiment_down = gr.Button(\"üëé\")\n",
        "\n",
        "                # Intent Row\n",
        "                intent_output = gr.Textbox(\n",
        "                    label=\"Intent\",\n",
        "                    interactive=False\n",
        "                )\n",
        "                with gr.Row():\n",
        "                    intent_up = gr.Button(\"üëç\")\n",
        "                    intent_down = gr.Button(\"üëé\")\n",
        "\n",
        "                # Topic Row\n",
        "                topic_output = gr.Textbox(\n",
        "                    label=\"Topic\",\n",
        "                    interactive=False\n",
        "                )\n",
        "                with gr.Row():\n",
        "                    topic_up = gr.Button(\"üëç\")\n",
        "                    topic_down = gr.Button(\"üëé\")\n",
        "\n",
        "                # Answer Row\n",
        "                answer_output = gr.Textbox(\n",
        "                    label=\"Answer\",\n",
        "                    interactive=False\n",
        "                )\n",
        "                with gr.Row():\n",
        "                    answer_up = gr.Button(\"üëç\")\n",
        "                    answer_down = gr.Button(\"üëé\")\n",
        "\n",
        "        # Event handlers\n",
        "        submit_btn.click(\n",
        "            analyze_question,\n",
        "            inputs=[question_input, chatbot],\n",
        "            outputs=[question_input, chatbot, sentiment_output, intent_output, topic_output, answer_output, response_time_output, refined_question, final_rating_output]\n",
        "        )\n",
        "\n",
        "        question_input.submit(\n",
        "            analyze_question,\n",
        "            inputs=[question_input, chatbot],\n",
        "            outputs=[question_input, chatbot, sentiment_output, intent_output, topic_output, answer_output, response_time_output, refined_question, final_rating_output]\n",
        "        )\n",
        "\n",
        "        clear_btn.click(\n",
        "            clear_inputs,\n",
        "            outputs=[question_input, chatbot, sentiment_output, intent_output, topic_output, answer_output, response_time_output, final_rating_output, refined_question]\n",
        "        )\n",
        "\n",
        "        end_conversation_btn.click(\n",
        "            end_conversation,\n",
        "            outputs=[question_input, refined_question, chatbot, sentiment_output, intent_output, topic_output, answer_output,response_time_output, final_rating_output]\n",
        "        )\n",
        "\n",
        "        # Left panel feedback handlers\n",
        "        chat_thumbs_up.click(\n",
        "            lambda: capture_feedback_final_response(\"chat\", \"positive\"),\n",
        "            outputs=gr.Textbox(visible=False)\n",
        "        )\n",
        "        chat_thumbs_down.click(\n",
        "            lambda: capture_feedback_final_response(\"chat\", \"negative\"),\n",
        "            outputs=gr.Textbox(visible=False)\n",
        "        )\n",
        "\n",
        "        # Right panel feedback handlers\n",
        "        # Sentiment feedback\n",
        "        sentiment_up.click(\n",
        "            lambda: capture_feedback_sentiment(\"sentiment\", \"positive\"),\n",
        "            outputs=gr.Textbox(visible=False)\n",
        "        )\n",
        "        sentiment_down.click(\n",
        "            lambda: capture_feedback_sentiment(\"sentiment\", \"negative\"),\n",
        "            outputs=gr.Textbox(visible=False)\n",
        "        )\n",
        "\n",
        "        # Intent feedback\n",
        "        intent_up.click(\n",
        "            lambda: capture_feedback_intent(\"intent\", \"positive\"),\n",
        "            outputs=gr.Textbox(visible=False)\n",
        "        )\n",
        "        intent_down.click(\n",
        "            lambda: capture_feedback_intent(\"intent\", \"negative\"),\n",
        "            outputs=gr.Textbox(visible=False)\n",
        "        )\n",
        "\n",
        "        # Topic feedback\n",
        "        topic_up.click(\n",
        "            lambda: capture_feedback_topic(\"topic\", \"positive\"),\n",
        "            outputs=gr.Textbox(visible=False)\n",
        "        )\n",
        "        topic_down.click(\n",
        "            lambda: capture_feedback_topic(\"topic\", \"negative\"),\n",
        "            outputs=gr.Textbox(visible=False)\n",
        "        )\n",
        "\n",
        "        # Answer feedback\n",
        "        answer_up.click(\n",
        "            lambda: capture_feedback_current_response(\"answer\", \"positive\"),\n",
        "            outputs=gr.Textbox(visible=False)\n",
        "        )\n",
        "        answer_down.click(\n",
        "            lambda: capture_feedback_current_response(\"answer\", \"negative\"),\n",
        "            outputs=gr.Textbox(visible=False)\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZH1CU4PS7xp",
        "outputId": "6ed6ebdf-ae88-449a-a32a-ee0c2ccd51f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-ce96cee412ed>:27: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(temperature=1, model=\"gpt-3.5-turbo\", openai_api_key=token)\n",
            "/content/drive/MyDrive/IISC_Capstone_Project/response_agent.py:13: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  self.memory = ConversationBufferMemory(input_key='query')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'Cancellation Fees', 1: 'Contact Details', 2: 'Cost and Budget Planning', 3: 'Booking Assistance', 4: 'Packing and Travel Tips', 5: 'Customer Support or Troubleshooting', 6: 'Accommodation Details', 7: 'Travel Documentation', 8: 'Flight and Boarding Information', 9: 'Travel Itineraries', 10: 'Travel Advice'}\n",
            "{0: 'Neutral', 1: 'Positive', 2: 'Negative'}\n",
            "{0: 'Lodging', 1: 'Travel Advice', 2: 'Boarding'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/IISC_Capstone_Project/input_analysis_agent.py:116: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. See LangGraph documentation for more details: https://langchain-ai.github.io/langgraph/. Refer here for its pre-built ReAct agent: https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/\n",
            "  self.agent = initialize_agent(self.tools,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "tp0jUC11T3dX",
        "outputId": "bf46b603-b320-4d29-ffc7-40f62491921f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c3e85a877c1242b1c1.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c3e85a877c1242b1c1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}